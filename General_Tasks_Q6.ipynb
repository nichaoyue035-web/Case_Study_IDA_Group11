{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2dc704-07be-48f2-92e3-4d7ceef66222",
   "metadata": {},
   "source": [
    "## Task6:Hit and Run Accident Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658149a1-6f59-45c4-b066-92d3df54aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 扫描 Data 目录以定位零件…（会打印命中文件、列名和命中次数）\n",
      "    [命中] Fahrzeug\\Bestandteile_Fahrzeuge_OEM1_Typ12.csv | part列=ID_Karosserie | 车辆列=ID_Fahrzeug | 命中=1\n",
      "    [命中] Komponente\\Bestandteile_Komponente_K5.csv | part列=ID_K5 | 车辆列=None | 命中=1\n",
      "    [命中] Komponente\\Komponente_K5.csv | part列=ID_Karosserie.x | 车辆列=None | 命中=1\n",
      "\n",
      ">>> 共命中 3 个文件；匹配到车辆ID数量：1（示例） ['12-1-12-82']\n",
      "\n",
      ">>> 在注册表中筛选事故当天有效记录（带进度）…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 600,000 行…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 1,000,000 行…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 1,600,000 行…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 2,000,000 行…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 2,600,000 行…\n",
      "    [进度] Zulassungen_alle_Fahrzeuge.csv 已处理 3,000,000 行…\n",
      "\n",
      "=== 事故当天的注册地结果（前20行） ===\n",
      "  Fahrzeug_ID Zulassungsbezirk Zulassungsbeginn Abmeldedatum\n",
      "0  12-1-12-82     ASCHERSLEBEN       2009-01-02          NaT\n",
      "\n",
      "✅ 结果已保存：D:\\IDA\\Case_Study_IDA_Group11\\Data\\outputs\\Task6_result.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================= Task 6：肇事逃逸调查（最终版：含进度 & 智能日期解析 & 静默告警） =======================\n",
    "# 目标：查询 2010-08-11 当天，零件 K5-112-1122-79 对应车辆的注册地\n",
    "# 路径固定：D:\\IDA\\Case_Study_IDA_Group11\\Data\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re, csv, warnings\n",
    "\n",
    "# ---- 静默与显示设置（去除烦人的日期解析告警） ----\n",
    "warnings.filterwarnings(\"ignore\", message=\"Parsing dates in %Y-%m-%d format\")\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "\n",
    "# ---------- 基本配置 ----------\n",
    "BASE_DIR = Path(r\"D:\\IDA\\Case_Study_IDA_Group11\\Data\")\n",
    "TARGET_PART_RAW = \"K5-112-1122-79\"\n",
    "QUERY_DAY = pd.to_datetime(\"2010-08-11\")\n",
    "CHUNKSIZE = 200_000  # 分块读取大小\n",
    "\n",
    "# ---------- 列名候选（含你数据里的真实命名） ----------\n",
    "VEH_ID_CANDS = [\n",
    "    \"idnummer\",\"fahrzeug_id\",\"fahrzeugid\",\"vehicle_id\",\"vehicleid\",\"vin\",\n",
    "    \"fahrzeug\",\"fahrzeugnr\",\"id_fahrzeug\",\"fahrzeugnummer\",\"fz_id\",\"fzg_id\",\"fzgid\"\n",
    "]\n",
    "REG_START_CANDS = [\"zulassung\",\"zulassungsdatum\",\"anmeldedatum\",\"start\",\"begin\"]\n",
    "REG_END_CANDS   = [\"abmeldedatum\",\"abmeldung\",\"ende\",\"end\"]\n",
    "REG_REGION_CANDS= [\"gemeinden\",\"gemeinde\",\"zulassungsbezirk\",\"ort\",\"region\",\"kreis\",\"amt\",\"bezirk\"]\n",
    "\n",
    "PART_CANDS = [\n",
    "    \"teilnummer\",\"teilenummer\",\"partno\",\"part_number\",\"teilenr\",\n",
    "    \"komponente\",\"bauteil\",\"karosserieteil\",\"bodypart\",\"einzelteil\",\"einzelteil_id\",\"einzelteilnummer\",\n",
    "    \"id_karosserie\",\"id_k5\",\"id_karosserie.x\"   # 结合你日志中的命中列\n",
    "]\n",
    "\n",
    "# ---------- 工具函数 ----------\n",
    "def canon_text(s: str) -> str:\n",
    "    \"\"\"标准化零件号：统一破折号、去非字母数字、转大写。\"\"\"\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().upper()\n",
    "    s = s.replace(\"–\",\"-\").replace(\"—\",\"-\").replace(\"−\",\"-\").replace(\"‐\",\"-\")\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", s)\n",
    "\n",
    "TARGET_PART_CANON = canon_text(TARGET_PART_RAW)\n",
    "\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"ä\",\"a\").replace(\"ö\",\"o\").replace(\"ü\",\"u\").replace(\"ß\",\"ss\")\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", s)\n",
    "\n",
    "def first_match_col(cols, cands):\n",
    "    cols = [str(c) for c in cols]\n",
    "    mp = {norm_col(c): c for c in cols}\n",
    "    # 完全匹配\n",
    "    for cand in cands:\n",
    "        k = norm_col(cand)\n",
    "        if k in mp:\n",
    "            return mp[k]\n",
    "    # 宽松包含\n",
    "    for cand in cands:\n",
    "        k = norm_col(cand)\n",
    "        for nk, orig in mp.items():\n",
    "            if k in nk:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def sniff_sep(filepath):\n",
    "    \"\"\"尝试猜 CSV 分隔符；失败返回 None。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", newline=\"\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(4096)\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=\",;\\t|\")\n",
    "        return dialect.delimiter\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_read_csv(path, usecols=None, nrows=None, chunksize=None, dtype=str):\n",
    "    \"\"\"\n",
    "    稳健读取 CSV：engine='python' 自动推断 → sniff → ';' → ','。\n",
    "    成功返回 DataFrame 或 迭代器；失败返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, usecols=usecols, nrows=nrows, chunksize=chunksize,\n",
    "                           dtype=dtype, low_memory=False, engine=\"python\", sep=None)\n",
    "    except Exception:\n",
    "        sep = sniff_sep(path)\n",
    "        for sp in [sep, \";\", \",\"]:\n",
    "            if not sp: \n",
    "                continue\n",
    "            try:\n",
    "                return pd.read_csv(path, usecols=usecols, nrows=nrows, chunksize=chunksize,\n",
    "                                   dtype=dtype, low_memory=False, sep=sp)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "def preview_columns(path):\n",
    "    try:\n",
    "        if path.suffix.lower()==\".csv\":\n",
    "            df = safe_read_csv(path, nrows=5)\n",
    "            if df is None: return []\n",
    "            if hasattr(df, \"__iter__\") and not isinstance(df, pd.DataFrame):\n",
    "                df = next(df)\n",
    "            return [str(c) for c in df.columns]\n",
    "        else:\n",
    "            df = pd.read_excel(path, nrows=5, engine=None)\n",
    "            return [str(c) for c in df.columns]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def iter_all_files(root: Path):\n",
    "    \"\"\"全目录递归找 CSV / XLS / XLSX。\"\"\"\n",
    "    return list(root.rglob(\"*.csv\")) + list(root.rglob(\"*.xls\")) + list(root.rglob(\"*.xlsx\"))\n",
    "\n",
    "def parse_date_series(s):\n",
    "    \"\"\"\n",
    "    智能日期解析：\n",
    "    1) 先按 YYYY-MM-DD 解析；\n",
    "    2) 对没解析成功的，再用 dayfirst=True 兜底（如 DD.MM.YYYY / DD/MM/YYYY）。\n",
    "    \"\"\"\n",
    "    s1 = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    if isinstance(s1, pd.Series) and s1.isna().any():\n",
    "        idx = s1.isna()\n",
    "        s1.loc[idx] = pd.to_datetime(s[idx], errors=\"coerce\", dayfirst=True)\n",
    "    return s1\n",
    "\n",
    "# ================== A. 全目录扫描：找到包含目标零件的文件 → 抽取车辆ID ==================\n",
    "print(\">>> 扫描 Data 目录以定位零件…（会打印命中文件、列名和命中次数）\")\n",
    "veh_ids = set()\n",
    "hits_detail = []\n",
    "\n",
    "all_files = iter_all_files(BASE_DIR)\n",
    "# 把 Zulassungen 相关文件放到后面，避免误当零件表\n",
    "all_files = sorted(all_files, key=lambda p: (\"zulass\" in p.as_posix().lower(), p.name.lower()))\n",
    "\n",
    "for f in all_files:\n",
    "    cols = preview_columns(f)\n",
    "    if not cols: \n",
    "        continue\n",
    "    part_col = first_match_col(cols, PART_CANDS)\n",
    "    veh_col  = first_match_col(cols, VEH_ID_CANDS)\n",
    "\n",
    "    try:\n",
    "        if f.suffix.lower()==\".csv\":\n",
    "            local_cnt = 0\n",
    "            # 1) 先用“猜到的 part 列”直接查\n",
    "            if part_col:\n",
    "                it = safe_read_csv(f, usecols=[part_col]+([veh_col] if veh_col else []),\n",
    "                                   chunksize=CHUNKSIZE, dtype=str)\n",
    "                if it is not None:\n",
    "                    for chunk in it:\n",
    "                        chunk = chunk.fillna(\"\")\n",
    "                        m = chunk[part_col].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                        if m.any():\n",
    "                            local_cnt += int(m.sum())\n",
    "                            if veh_col and veh_col in chunk.columns:\n",
    "                                veh_ids.update(chunk.loc[m, veh_col].astype(str))\n",
    "            # 2) 若未命中或没识别到 part 列，宽松扫描所有字符串列\n",
    "            if local_cnt==0:\n",
    "                head = safe_read_csv(f, nrows=1000)\n",
    "                if head is not None:\n",
    "                    if hasattr(head, \"__iter__\") and not isinstance(head, pd.DataFrame):\n",
    "                        head = next(head)\n",
    "                    str_cols = [c for c in head.columns if head[c].dtype==object]\n",
    "                    if str_cols:\n",
    "                        it = safe_read_csv(f, usecols=str_cols, chunksize=CHUNKSIZE, dtype=str)\n",
    "                        if it is not None:\n",
    "                            col_hits = {}\n",
    "                            for chunk in it:\n",
    "                                chunk = chunk.fillna(\"\")\n",
    "                                for c in str_cols:\n",
    "                                    m = chunk[c].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                                    if m.any():\n",
    "                                        cnt = int(m.sum())\n",
    "                                        col_hits[c] = col_hits.get(c,0)+cnt\n",
    "                                        if not veh_col:\n",
    "                                            veh_col = first_match_col(chunk.columns, VEH_ID_CANDS)\n",
    "                                        if veh_col and veh_col in chunk.columns:\n",
    "                                            veh_ids.update(chunk.loc[m, veh_col].astype(str))\n",
    "                                        local_cnt += cnt\n",
    "                            if local_cnt:\n",
    "                                part_col = max(col_hits.items(), key=lambda kv: kv[1])[0]\n",
    "            if local_cnt:\n",
    "                hits_detail.append((f, part_col, veh_col, local_cnt))\n",
    "                print(f\"    [命中] {f.relative_to(BASE_DIR)} | part列={part_col} | 车辆列={veh_col} | 命中={local_cnt}\")\n",
    "\n",
    "        else:\n",
    "            # Excel\n",
    "            df = pd.read_excel(f, engine=None).fillna(\"\")\n",
    "            local_cnt = 0\n",
    "            if part_col and part_col in df.columns:\n",
    "                m = df[part_col].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                local_cnt = int(m.sum())\n",
    "                if local_cnt and veh_col and veh_col in df.columns:\n",
    "                    veh_ids.update(df.loc[m, veh_col].astype(str))\n",
    "            if local_cnt==0:\n",
    "                for c in df.columns:\n",
    "                    if df[c].dtype==object:\n",
    "                        m = df[c].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                        if m.any():\n",
    "                            cnt = int(m.sum()); local_cnt += cnt; part_col = c\n",
    "                            if not veh_col:\n",
    "                                veh_col = first_match_col(df.columns, VEH_ID_CANDS)\n",
    "                            if veh_col and veh_col in df.columns:\n",
    "                                veh_ids.update(df.loc[m, veh_col].astype(str))\n",
    "            if local_cnt:\n",
    "                hits_detail.append((f, part_col, veh_col, local_cnt))\n",
    "                print(f\"    [命中] {f.relative_to(BASE_DIR)} | part列={part_col} | 车辆列={veh_col} | 命中={local_cnt}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    [扫描失败] {f.relative_to(BASE_DIR)} -> {e}\")\n",
    "\n",
    "veh_ids = sorted([v for v in map(str, veh_ids) if v and v.lower()!=\"nan\"])\n",
    "print(f\"\\n>>> 共命中 {len(hits_detail)} 个文件；匹配到车辆ID数量：{len(veh_ids)}（示例） {veh_ids[:10]}\")\n",
    "if not veh_ids:\n",
    "    raise SystemExit(\"❌ 没找到包含该零件的记录。\")\n",
    "\n",
    "# ================== B. 在注册表里筛选（适配：IDNummer / Gemeinden / Zulassung） ==================\n",
    "reg_path = BASE_DIR / \"Zulassungen\" / \"Zulassungen_alle_Fahrzeuge.csv\"\n",
    "if not reg_path.exists():\n",
    "    raise SystemExit(f\"❌ 找不到注册表：{reg_path}\")\n",
    "\n",
    "cols_reg = preview_columns(reg_path)\n",
    "col_fzg   = first_match_col(cols_reg, VEH_ID_CANDS)     # e.g., IDNummer\n",
    "col_start = first_match_col(cols_reg, REG_START_CANDS)  # e.g., Zulassung\n",
    "col_end   = first_match_col(cols_reg, REG_END_CANDS)    # 可能不存在\n",
    "col_reg   = first_match_col(cols_reg, REG_REGION_CANDS) # e.g., Gemeinden\n",
    "\n",
    "if not col_fzg or not col_start or not col_reg:\n",
    "    raise SystemExit(f\"❌ 注册表关键列识别失败。列名预览：{cols_reg}\")\n",
    "\n",
    "print(\"\\n>>> 在注册表中筛选事故当天有效记录（带进度）…\")\n",
    "rows = []\n",
    "usecols = [col_fzg, col_start, col_reg] + ([col_end] if col_end else [])\n",
    "it = safe_read_csv(reg_path, usecols=usecols, chunksize=CHUNKSIZE, dtype=str)\n",
    "if it is None:\n",
    "    raise SystemExit(\"❌ 注册表读取失败（分隔符或编码问题）。\")\n",
    "\n",
    "count = 0\n",
    "for chunk in it:\n",
    "    count += len(chunk)\n",
    "    if count % 500_000 < CHUNKSIZE:\n",
    "        print(f\"    [进度] {reg_path.name} 已处理 {count:,} 行…\")\n",
    "\n",
    "    # 智能日期解析（避免告警 & 兼容多格式）\n",
    "    chunk[col_start] = parse_date_series(chunk[col_start])\n",
    "    if col_end and col_end in chunk.columns:\n",
    "        end_series = parse_date_series(chunk[col_end])\n",
    "    else:\n",
    "        end_series = pd.NaT\n",
    "\n",
    "    # 条件：start ≤ 查询日 ≤ end 或没有 end\n",
    "    if isinstance(end_series, pd.Series):\n",
    "        mask_end_ok = (end_series.isna()) | (end_series >= QUERY_DAY)\n",
    "        mask = (chunk[col_fzg].astype(str).isin(veh_ids) &\n",
    "                (chunk[col_start] <= QUERY_DAY) & mask_end_ok)\n",
    "        hit = chunk.loc[mask, [col_fzg, col_reg, col_start]].copy()\n",
    "        hit[\"Abmeldedatum\"] = end_series.loc[mask].values\n",
    "    else:\n",
    "        mask = (chunk[col_fzg].astype(str).isin(veh_ids) &\n",
    "                (chunk[col_start] <= QUERY_DAY))\n",
    "        hit = chunk.loc[mask, [col_fzg, col_reg, col_start]].copy()\n",
    "        hit[\"Abmeldedatum\"] = pd.NaT\n",
    "\n",
    "    if not hit.empty:\n",
    "        hit[\"source_file\"] = reg_path.relative_to(BASE_DIR).as_posix()\n",
    "        rows.append(hit)\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"❌ 在注册表中没有筛到任何 2010-08-11 有效记录。\")\n",
    "\n",
    "result = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# 同一车辆多条 → 取“最接近且不晚于查询日”的那条\n",
    "veh_col_any   = first_match_col(result.columns, VEH_ID_CANDS) or col_fzg\n",
    "start_col_any = first_match_col(result.columns, REG_START_CANDS) or col_start\n",
    "result = (result\n",
    "          .sort_values([veh_col_any, start_col_any], ascending=[True, False])\n",
    "          .drop_duplicates(subset=[veh_col_any]))\n",
    "\n",
    "# 统一列名 & 输出\n",
    "result = result.rename(columns={\n",
    "    veh_col_any: \"Fahrzeug_ID\",\n",
    "    first_match_col(result.columns, REG_REGION_CANDS) or col_reg: \"Zulassungsbezirk\",\n",
    "    start_col_any: \"Zulassungsbeginn\",\n",
    "})\n",
    "if \"Abmeldedatum\" not in result.columns:\n",
    "    result[\"Abmeldedatum\"] = pd.NaT\n",
    "\n",
    "print(\"\\n=== 事故当天的注册地结果（前20行） ===\")\n",
    "print(result[[\"Fahrzeug_ID\",\"Zulassungsbezirk\",\"Zulassungsbeginn\",\"Abmeldedatum\"]].head(20))\n",
    "\n",
    "# 保存\n",
    "out_dir = BASE_DIR / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "out_path = out_dir / \"Task6_result.csv\"\n",
    "result[[\"Fahrzeug_ID\",\"Zulassungsbezirk\",\"Zulassungsbeginn\",\"Abmeldedatum\",\"source_file\"]].to_csv(\n",
    "    out_path, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\n✅ 结果已保存：{out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286b51b-2b1b-4dca-904f-006af6653ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
