{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2dc704-07be-48f2-92e3-4d7ceef66222",
   "metadata": {},
   "source": [
    "## Task6:Hit and Run Accident Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658149a1-6f59-45c4-b066-92d3df54aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Projektwurzelverzeichnis: D:\\IDA\\Case_Study_IDA_Group11\n",
      "✅ Data-Verzeichnis: D:\\IDA\\Case_Study_IDA_Group11\\Data\n",
      ">>> Scanne Data-Verzeichnis zur Lokalisierung des Teils… (zeigt Trefferdateien, Spalten und Trefferanzahl)\n",
      "    [Treffer] Fahrzeug\\Bestandteile_Fahrzeuge_OEM1_Typ12.csv | Teil-Spalte=ID_Karosserie | Fahrzeug-Spalte=ID_Fahrzeug | Treffer=1\n",
      "    [Treffer] Komponente\\Bestandteile_Komponente_K5.csv | Teil-Spalte=ID_K5 | Fahrzeug-Spalte=None | Treffer=1\n",
      "    [Treffer] Komponente\\Komponente_K5.csv | Teil-Spalte=ID_Karosserie.x | Fahrzeug-Spalte=None | Treffer=1\n",
      "\n",
      ">>> Insgesamt 3 Dateien getroffen; Anzahl gefundener Fahrzeug-IDs: 1 (Beispiel) ['12-1-12-82']\n",
      "\n",
      ">>> Filtere gültige Zulassungen am Unfalltag (mit Fortschritt)…\n",
      "    [Fortschritt] Zulassungen_alle_Fahrzeuge.csv 1,000,000 Zeilen verarbeitet…\n",
      "    [Fortschritt] Zulassungen_alle_Fahrzeuge.csv 2,000,000 Zeilen verarbeitet…\n",
      "    [Fortschritt] Zulassungen_alle_Fahrzeuge.csv 3,000,000 Zeilen verarbeitet…\n",
      "\n",
      "=== Zulassungs-Ergebnisse am Unfalltag (Top 20) ===\n",
      "  Fahrzeug_ID Zulassungsbezirk Zulassungsbeginn Abmeldedatum\n",
      "0  12-1-12-82     ASCHERSLEBEN       2009-01-02          NaT\n",
      "\n",
      "✅ Ergebnis gespeichert: D:\\IDA\\Case_Study_IDA_Group11\\Data\\outputs\\Task6_result.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================= Task 6：肇事逃逸调查（相对路径+性能优化版） =======================\n",
    "# 目标：查询 2010-08-11 当天，零件 K5-112-1122-79 对应车辆的注册地\n",
    "# 优化点：在注册表中过滤时，先按车辆ID筛选，再解析日期（显著提速）\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re, csv, warnings\n",
    "\n",
    "# ---- 静默与显示设置（去除烦人的日期解析告警）----\n",
    "warnings.filterwarnings(\"ignore\", message=\"Parsing dates in %Y-%m-%d format\")\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "\n",
    "# ---------- 自动检测项目根目录 ----------\n",
    "def detect_project_root() -> Path:\n",
    "    here = Path.cwd()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"Data\").exists():   # 如果找到 Data 文件夹，就认为是项目根目录\n",
    "            return p\n",
    "    return here  # 找不到就用当前目录\n",
    "\n",
    "PROJECT_ROOT = detect_project_root()\n",
    "BASE_DIR = PROJECT_ROOT / \"Data\"   # 相对路径：统一指向项目下的 Data 文件夹\n",
    "\n",
    "print(\"✅ Projektwurzelverzeichnis:\", PROJECT_ROOT)\n",
    "print(\"✅ Data-Verzeichnis:\", BASE_DIR)\n",
    "\n",
    "# ---------- 基本配置 ----------\n",
    "TARGET_PART_RAW = \"K5-112-1122-79\"\n",
    "QUERY_DAY = pd.to_datetime(\"2010-08-11\")\n",
    "CHUNKSIZE = 200_000  # 分块读取大小\n",
    "\n",
    "# ---------- 列名候选（含你数据里的真实命名） ----------\n",
    "VEH_ID_CANDS = [\n",
    "    \"idnummer\",\"fahrzeug_id\",\"fahrzeugid\",\"vehicle_id\",\"vehicleid\",\"vin\",\n",
    "    \"fahrzeug\",\"fahrzeugnr\",\"id_fahrzeug\",\"fahrzeugnummer\",\"fz_id\",\"fzg_id\",\"fzgid\"\n",
    "]\n",
    "REG_START_CANDS = [\"zulassung\",\"zulassungsdatum\",\"anmeldedatum\",\"start\",\"begin\"]\n",
    "REG_END_CANDS   = [\"abmeldedatum\",\"abmeldung\",\"ende\",\"end\"]\n",
    "REG_REGION_CANDS= [\"gemeinden\",\"gemeinde\",\"zulassungsbezirk\",\"ort\",\"region\",\"kreis\",\"amt\",\"bezirk\"]\n",
    "\n",
    "PART_CANDS = [\n",
    "    \"teilnummer\",\"teilenummer\",\"partno\",\"part_number\",\"teilenr\",\n",
    "    \"komponente\",\"bauteil\",\"karosserieteil\",\"bodypart\",\"einzelteil\",\"einzelteil_id\",\"einzelteilnummer\",\n",
    "    \"id_karosserie\",\"id_k5\",\"id_karosserie.x\"   # 结合你日志中的命中列\n",
    "]\n",
    "\n",
    "# ---------- 工具函数 ----------\n",
    "def canon_text(s: str) -> str:\n",
    "    \"\"\"标准化零件号：统一破折号、去非字母数字、转大写。\"\"\"\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().upper()\n",
    "    s = s.replace(\"–\",\"-\").replace(\"—\",\"-\").replace(\"−\",\"-\").replace(\"‐\",\"-\")\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", s)\n",
    "\n",
    "TARGET_PART_CANON = canon_text(TARGET_PART_RAW)\n",
    "\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"ä\",\"a\").replace(\"ö\",\"o\").replace(\"ü\",\"u\").replace(\"ß\",\"ss\")\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", s)\n",
    "\n",
    "def first_match_col(cols, cands):\n",
    "    cols = [str(c) for c in cols]\n",
    "    mp = {norm_col(c): c for c in cols}\n",
    "    # 完全匹配\n",
    "    for cand in cands:\n",
    "        k = norm_col(cand)\n",
    "        if k in mp:\n",
    "            return mp[k]\n",
    "    # 宽松包含\n",
    "    for cand in cands:\n",
    "        k = norm_col(cand)\n",
    "        for nk, orig in mp.items():\n",
    "            if k in nk:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def sniff_sep(filepath):\n",
    "    \"\"\"尝试猜 CSV 分隔符；失败返回 None。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", newline=\"\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(4096)\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=\",;\\t|\")\n",
    "        return dialect.delimiter\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_read_csv(path, usecols=None, nrows=None, chunksize=None, dtype=str):\n",
    "    \"\"\"\n",
    "    稳健读取 CSV：engine='python' 自动推断 → sniff → ';' → ','。\n",
    "    成功返回 DataFrame 或 迭代器；失败返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, usecols=usecols, nrows=nrows, chunksize=chunksize,\n",
    "                           dtype=dtype, low_memory=False, engine=\"python\", sep=None)\n",
    "    except Exception:\n",
    "        sep = sniff_sep(path)\n",
    "        for sp in [sep, \";\", \",\", \"\\t\", \"|\"]:\n",
    "            if not sp: \n",
    "                continue\n",
    "            try:\n",
    "                return pd.read_csv(path, usecols=usecols, nrows=nrows, chunksize=chunksize,\n",
    "                                   dtype=dtype, low_memory=False, sep=sp)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "def preview_columns(path):\n",
    "    try:\n",
    "        if path.suffix.lower()==\".csv\":\n",
    "            df = safe_read_csv(path, nrows=5)\n",
    "            if df is None: return []\n",
    "            if hasattr(df, \"__iter__\") and not isinstance(df, pd.DataFrame):\n",
    "                df = next(df)\n",
    "            return [str(c) for c in df.columns]\n",
    "        else:\n",
    "            df = pd.read_excel(path, nrows=5, engine=None)\n",
    "            return [str(c) for c in df.columns]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def iter_all_files(root: Path):\n",
    "    \"\"\"全目录递归找 CSV / XLS / XLSX。\"\"\"\n",
    "    return list(root.rglob(\"*.csv\")) + list(root.rglob(\"*.xls\")) + list(root.rglob(\"*.xlsx\"))\n",
    "\n",
    "def parse_date_series(s):\n",
    "    \"\"\"\n",
    "    智能日期解析：\n",
    "    1) 先按 YYYY-MM-DD 解析；\n",
    "    2) 对没解析成功的，再用 dayfirst=True 兜底（如 DD.MM.YYYY / DD/MM/YYYY）。\n",
    "    \"\"\"\n",
    "    s1 = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    if isinstance(s1, pd.Series) and s1.isna().any():\n",
    "        idx = s1.isna()\n",
    "        s1.loc[idx] = pd.to_datetime(s[idx], errors=\"coerce\", dayfirst=True)\n",
    "    return s1\n",
    "\n",
    "# ================== A. 全目录扫描：找到包含目标零件的文件 → 抽取车辆ID ==================\n",
    "print(\">>> Scanne Data-Verzeichnis zur Lokalisierung des Teils… (zeigt Trefferdateien, Spalten und Trefferanzahl)\")\n",
    "veh_ids = set()\n",
    "hits_detail = []\n",
    "\n",
    "all_files = iter_all_files(BASE_DIR)\n",
    "# 把 Zulassungen 相关文件放到后面，避免误当零件表\n",
    "all_files = sorted(all_files, key=lambda p: (\"zulass\" in p.as_posix().lower(), p.name.lower()))\n",
    "\n",
    "for f in all_files:\n",
    "    cols = preview_columns(f)\n",
    "    if not cols: \n",
    "        continue\n",
    "    part_col = first_match_col(cols, PART_CANDS)\n",
    "    veh_col  = first_match_col(cols, VEH_ID_CANDS)\n",
    "\n",
    "    try:\n",
    "        if f.suffix.lower()==\".csv\":\n",
    "            local_cnt = 0\n",
    "            # 1) 先用“猜到的 part 列”直接查\n",
    "            if part_col:\n",
    "                it = safe_read_csv(f, usecols=[part_col]+([veh_col] if veh_col else []),\n",
    "                                   chunksize=CHUNKSIZE, dtype=str)\n",
    "                if it is not None:\n",
    "                    for chunk in it:\n",
    "                        chunk = chunk.fillna(\"\")\n",
    "                        m = chunk[part_col].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                        if m.any():\n",
    "                            local_cnt += int(m.sum())\n",
    "                            if veh_col and veh_col in chunk.columns:\n",
    "                                veh_ids.update(chunk.loc[m, veh_col].astype(str))\n",
    "            # 2) 若未命中或没识别到 part 列，宽松扫描所有字符串列\n",
    "            if local_cnt==0:\n",
    "                head = safe_read_csv(f, nrows=1000)\n",
    "                if head is not None:\n",
    "                    if hasattr(head, \"__iter__\") and not isinstance(head, pd.DataFrame):\n",
    "                        head = next(head)\n",
    "                    str_cols = [c for c in head.columns if head[c].dtype==object]\n",
    "                    if str_cols:\n",
    "                        it = safe_read_csv(f, usecols=str_cols, chunksize=CHUNKSIZE, dtype=str)\n",
    "                        if it is not None:\n",
    "                            col_hits = {}\n",
    "                            for chunk in it:\n",
    "                                chunk = chunk.fillna(\"\")\n",
    "                                for c in str_cols:\n",
    "                                    m = chunk[c].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                                    if m.any():\n",
    "                                        cnt = int(m.sum())\n",
    "                                        col_hits[c] = col_hits.get(c,0)+cnt\n",
    "                                        if not veh_col:\n",
    "                                            veh_col = first_match_col(chunk.columns, VEH_ID_CANDS)\n",
    "                                        if veh_col and veh_col in chunk.columns:\n",
    "                                            veh_ids.update(chunk.loc[m, veh_col].astype(str))\n",
    "                                        local_cnt += cnt\n",
    "                            if local_cnt:\n",
    "                                part_col = max(col_hits.items(), key=lambda kv: kv[1])[0]\n",
    "            if local_cnt:\n",
    "                hits_detail.append((f, part_col, veh_col, local_cnt))\n",
    "                print(f\"    [Treffer] {f.relative_to(BASE_DIR)} | Teil-Spalte={part_col} | Fahrzeug-Spalte={veh_col} | Treffer={local_cnt}\")\n",
    "\n",
    "        else:\n",
    "            # Excel\n",
    "            df = pd.read_excel(f, engine=None).fillna(\"\")\n",
    "            local_cnt = 0\n",
    "            if part_col and part_col in df.columns:\n",
    "                m = df[part_col].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                local_cnt = int(m.sum())\n",
    "                if local_cnt and veh_col and veh_col in df.columns:\n",
    "                    veh_ids.update(df.loc[m, veh_col].astype(str))\n",
    "            if local_cnt==0:\n",
    "                for c in df.columns:\n",
    "                    if df[c].dtype==object:\n",
    "                        m = df[c].astype(str).map(canon_text).eq(TARGET_PART_CANON)\n",
    "                        if m.any():\n",
    "                            cnt = int(m.sum()); local_cnt += cnt; part_col = c\n",
    "                            if not veh_col:\n",
    "                                veh_col = first_match_col(df.columns, VEH_ID_CANDS)\n",
    "                            if veh_col and veh_col in df.columns:\n",
    "                                veh_ids.update(df.loc[m, veh_col].astype(str))\n",
    "            if local_cnt:\n",
    "                hits_detail.append((f, part_col, veh_col, local_cnt))\n",
    "                print(f\"    [Treffer] {f.relative_to(BASE_DIR)} | Teil-Spalte={part_col} | Fahrzeug-Spalte={veh_col} | Treffer={local_cnt}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    [Scan-Fehler] {f.relative_to(BASE_DIR)} -> {e}\")\n",
    "\n",
    "veh_ids = sorted([v for v in map(str, veh_ids) if v and v.lower()!=\"nan\"])\n",
    "print(f\"\\n>>> Insgesamt {len(hits_detail)} Dateien getroffen; Anzahl gefundener Fahrzeug-IDs: {len(veh_ids)} (Beispiel) {veh_ids[:10]}\")\n",
    "if not veh_ids:\n",
    "    raise SystemExit(\"❌ Keine Aufzeichnungen mit diesem Teil gefunden.\")\n",
    "\n",
    "# ================== B. 在注册表里筛选（先按ID过滤，再解析日期 —— 提速关键） ==================\n",
    "reg_path = BASE_DIR / \"Zulassungen\" / \"Zulassungen_alle_Fahrzeuge.csv\"\n",
    "if not reg_path.exists():\n",
    "    raise SystemExit(f\"❌ Zulassungsdatei nicht gefunden: {reg_path}\")\n",
    "\n",
    "cols_reg = preview_columns(reg_path)\n",
    "col_fzg   = first_match_col(cols_reg, VEH_ID_CANDS)     # e.g., IDNummer\n",
    "col_start = first_match_col(cols_reg, REG_START_CANDS)  # e.g., Zulassung\n",
    "col_end   = first_match_col(cols_reg, REG_END_CANDS)    # 可能不存在\n",
    "col_reg   = first_match_col(cols_reg, REG_REGION_CANDS) # e.g., Gemeinden\n",
    "\n",
    "if not col_fzg or not col_start or not col_reg:\n",
    "    raise SystemExit(f\"❌ Erkennung der Schlüsselspalten fehlgeschlagen. Spaltenvorschau: {cols_reg}\")\n",
    "\n",
    "print(\"\\n>>> Filtere gültige Zulassungen am Unfalltag (mit Fortschritt)…\")\n",
    "rows = []\n",
    "usecols = [col_fzg, col_start, col_reg] + ([col_end] if col_end else [])\n",
    "it = safe_read_csv(reg_path, usecols=usecols, chunksize=CHUNKSIZE, dtype=str)\n",
    "if it is None:\n",
    "    raise SystemExit(\"❌ Zulassungsdatei konnte nicht gelesen werden (Trennzeichen/Encoding).\")\n",
    "\n",
    "veh_ids_set = set(veh_ids)  # 仅构建一次集合，加速 isin 判断\n",
    "count = 0\n",
    "for chunk in it:\n",
    "    count += len(chunk)\n",
    "    # 降低打印频率：每处理整百万行提示一次\n",
    "    if count // 1_000_000 != (count - len(chunk)) // 1_000_000:\n",
    "        print(f\"    [Fortschritt] {reg_path.name} {count:,} Zeilen verarbeitet…\")\n",
    "\n",
    "    # 先把可能用到的列变为 string，避免反复 astype\n",
    "    for c in [col_fzg, col_start] + ([col_end] if col_end else []) + [col_reg]:\n",
    "        if c in chunk.columns:\n",
    "            chunk[c] = chunk[c].astype(\"string\")\n",
    "\n",
    "    # ① 先按车辆ID过滤（大幅减少行数）\n",
    "    mask_id = chunk[col_fzg].isin(veh_ids_set)\n",
    "    if not mask_id.any():\n",
    "        continue\n",
    "\n",
    "    # ② 只对命中ID的行解析日期（显著提速）\n",
    "    starts = parse_date_series(chunk.loc[mask_id, col_start])\n",
    "    if col_end and col_end in chunk.columns:\n",
    "        ends = parse_date_series(chunk.loc[mask_id, col_end])\n",
    "    else:\n",
    "        ends = None\n",
    "\n",
    "    # ③ 组条件：start ≤ 查询日 ≤ end(或无end)\n",
    "    if ends is not None:\n",
    "        mask_end_ok = ends.isna() | (ends >= QUERY_DAY)\n",
    "        mask_day = (starts <= QUERY_DAY) & mask_end_ok\n",
    "    else:\n",
    "        mask_day = (starts <= QUERY_DAY)\n",
    "\n",
    "    if not mask_day.any():\n",
    "        continue\n",
    "\n",
    "    # ④ 取最终命中行\n",
    "    hit_idx = chunk.index[mask_id][mask_day]\n",
    "    hit = chunk.loc[hit_idx, [col_fzg, col_reg, col_start]].copy()\n",
    "    hit[\"Abmeldedatum\"] = ends.loc[mask_day].values if ends is not None else pd.NaT\n",
    "    hit[\"source_file\"] = reg_path.relative_to(BASE_DIR).as_posix()\n",
    "    rows.append(hit)\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"❌ Keine gültigen Zulassungsdatensätze am 2010-08-11 gefunden.\")\n",
    "\n",
    "result = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# 同一车辆多条 → 取“最接近且不晚于查询日”的那条\n",
    "veh_col_any   = first_match_col(result.columns, VEH_ID_CANDS) or col_fzg\n",
    "start_col_any = first_match_col(result.columns, REG_START_CANDS) or col_start\n",
    "result = (result\n",
    "          .sort_values([veh_col_any, start_col_any], ascending=[True, False])\n",
    "          .drop_duplicates(subset=[veh_col_any]))\n",
    "\n",
    "# 统一列名 & 输出\n",
    "result = result.rename(columns={\n",
    "    veh_col_any: \"Fahrzeug_ID\",\n",
    "    first_match_col(result.columns, REG_REGION_CANDS) or col_reg: \"Zulassungsbezirk\",\n",
    "    start_col_any: \"Zulassungsbeginn\",\n",
    "})\n",
    "if \"Abmeldedatum\" not in result.columns:\n",
    "    result[\"Abmeldedatum\"] = pd.NaT\n",
    "\n",
    "print(\"\\n=== Zulassungs-Ergebnisse am Unfalltag (Top 20) ===\")\n",
    "print(result[[\"Fahrzeug_ID\",\"Zulassungsbezirk\",\"Zulassungsbeginn\",\"Abmeldedatum\"]].head(20))\n",
    "\n",
    "# 保存\n",
    "out_dir = BASE_DIR / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "out_path = out_dir / \"Task6_result.csv\"\n",
    "result[[\"Fahrzeug_ID\",\"Zulassungsbezirk\",\"Zulassungsbeginn\",\"Abmeldedatum\",\"source_file\"]].to_csv(\n",
    "    out_path, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\n✅ Ergebnis gespeichert: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286b51b-2b1b-4dca-904f-006af6653ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
